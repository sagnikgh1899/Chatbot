{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PARAMETERS USED:\n",
    "<b>Dataset and K varies</b> <br>\n",
    "\n",
    "##### These parameters are tuned to best possible extent\n",
    "1. thresh (opt = 0.3)\n",
    "2. alpha_weight_avg (opt = 0.6)\n",
    "3. per in LLE (opt = 0.5)\n",
    "4. percent in GetCutOffDistMod (opt = 0.7)\n",
    "5. val2 in Correcting_wrong_clustering (opt = 20)\n",
    "6. val3 in Correcting_wrong_clustering (opt = 15)\n",
    "7. percent1 in Correcting_wrong_clustering (opt = 0.8)\n",
    "8. percent2 in Correcting_wrong_clustering (opt = 0.16)\n",
    "9. percent3 in Correcting_wrong_clustering (opt = 0.10)\n",
    "10. percent4 in Correcting_wrong_clustering (opt = 0.01)\n",
    "11. algo for NN = kd_tree\n",
    "13. KVal in Outlier Postprocessing (opt = 3% of dataset size)\n",
    "14. percent5 in Outlier Postprocessing (opt = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import csv\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from statistics import mean\n",
    "from operator import itemgetter\n",
    "from collections import deque\n",
    "from collections import defaultdict\n",
    "from validclust import dunn\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn .metrics import davies_bouldin_score\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r\"E:\\ISI\\ISI PROJECT\\DATASETS 28-10-2020\\lympho(data).csv\"\n",
    "Ground_Truth_data_path=r\"E:\\ISI\\ISI PROJECT\\DATASETS 28-10-2020\\lympho(gt).csv\"\n",
    "K = 15            ## Actually K-1 neighbors are considered. Remember              #### Imp\n",
    "\n",
    "\n",
    "with open(data_path, 'r') as f:\n",
    "    data1 = np.genfromtxt(f, delimiter=',')   \n",
    "Dimension = data1.shape[1]\n",
    "#data1=data1/np.linalg.norm(data1)\n",
    "print(len(data1))\n",
    "print(\"Dimension: \",Dimension)\n",
    "thresh = 0.3                                                                       #### Imp\n",
    "alpha_weight_avg = 0.6\n",
    "algo = 'kd_tree' #'ball_tree'\n",
    "\n",
    "Outlier_Detection = True #False\n",
    "Clustering = False #True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the nearest neighbors, the density of point_i and density of its K neighbors\n",
    "nbrs_ddcal = NearestNeighbors(n_neighbors=K, algorithm=algo).fit(data1)\n",
    "distances_ddcal, indices_ddcal = nbrs_ddcal.kneighbors(data1)\n",
    "# print(indices_ddcal)\n",
    "\n",
    "Hash_Map = {i:0 for i in range(len(data1))}\n",
    "labels = {i:0 for i in range(len(data1))}\n",
    "density = []\n",
    "for i in range(len(data1)):\n",
    "    density.append([sum(distances_ddcal[i])/(K-1), data1[i], max(distances_ddcal[i]), indices_ddcal[i][1:], i])\n",
    "    Hash_Map[i] = density[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance-Density Value; Datapoint; Radius; k-nearest neighbors; idx_number\n",
    "# 0: 0.45; (25,5); 0.2; [1,2,3,4,5]; 0\n",
    "\n",
    "density.sort(key = itemgetter(0), reverse = False)\n",
    "#print(density[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the weighted moving average\n",
    "def numpy_ewma_vectorized_v2(values, alpha):\n",
    "    #values = np.array(values)\n",
    "    span = (2/alpha) - 1\n",
    "    df = pd.DataFrame(values)\n",
    "    return df.ewm(span=span).mean().iloc[-1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLE: Locally linear Embedding\n",
    "def LLE(data, K, data_point_Idx):\n",
    "    row=len(data)\n",
    "    #print(row)\n",
    "    dimension=len(data[0])\n",
    "    #print(dimension)\n",
    "    neighbors= K\n",
    "    W=np.zeros((row, row))\n",
    "\n",
    "    hash_map = {value:np.array([]) for value in range(len(data))}\n",
    "\n",
    "    for i in range(row):\n",
    "        D_i=np.array(data-data[i, :])\n",
    "        #print(D_i)\n",
    "        distance=(D_i**2).sum(1)\n",
    "        #print(distance)\n",
    "        nearest_neighbor=np.argsort(distance)[1:(neighbors+1)]\n",
    "        #print(nearest_neighbor)\n",
    "        D_nbrs=D_i[nearest_neighbor, :]\n",
    "        #print(D_nbrs)\n",
    "        Q=np.dot(D_nbrs, D_nbrs.T)\n",
    "        #print(Q)\n",
    "        t=np.trace(Q)\n",
    "        r=0.001*t\n",
    "        if(neighbors>=dimension):\n",
    "            Q=Q+(r*np.identity(neighbors))\n",
    "        w=np.linalg.solve(Q, np.ones(neighbors))\n",
    "        w=w/sum(w)\n",
    "        #print(i, w)\n",
    "        W[i, nearest_neighbor]=w\n",
    "\n",
    "        # My code\n",
    "        temp = []\n",
    "        for ele in w:\n",
    "            #print(ele)\n",
    "            #if ele != 0:\n",
    "            temp.append(ele)\n",
    "            temp.sort(reverse = True)\n",
    "            hash_map[i] = np.array(temp) \n",
    "            \n",
    "#     I=np.identity(row)\n",
    "#     M=I-W\n",
    "\n",
    "    per = 0.5 #0.6 #0.3 #0.5 #0.2 #0.05 #0.1\n",
    "    store_K = defaultdict()\n",
    "\n",
    "    for j in range(len(data)):\n",
    "        count = 0\n",
    "        for i in range(1, len(hash_map[j])):\n",
    "            #print(((1/neighbors) - (dec_per*(1/neighbors))), ((1/neighbors) + (dec_per*(1/neighbors))))\n",
    "            if round((abs(hash_map[j][i-1] - hash_map[j][i])/abs(hash_map[j][i-1]))*100,1) <= per:\n",
    "                count += 1\n",
    "        store_K[j] = count\n",
    "        \n",
    "    # print(store_K)\n",
    "    return store_K[data_point_Idx] + 1    # +1 because all neighbouring algorithms consider the datapoint itself to be a part\n",
    "                                          # of its neighbour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# DFS Based Approach:\n",
    "\n",
    "# However due to the maximum recursion depth constraint in Python Programming language\n",
    "# this is not a very good approach. The BFS one above is a better solution.\n",
    "# Typically it will fail for large datasets. \n",
    "# \"\"\"\n",
    "\n",
    "# def NovelClus(Parent, Hash_Map, idx, dataset, K, list_of_parents, labels, thresh, cluster_num, can_form_cluster):\n",
    "#     #print(Parent[1], idx)\n",
    "    \n",
    "#     print(\"Cluster =\", cluster_num)\n",
    "\n",
    "#     while idx <= K-2:\n",
    "\n",
    "#         Density_Parent = Parent[0]\n",
    "#         Radius = Parent[2]\n",
    "#         Child_Idx = Parent[3][idx] \n",
    "#         Child = Hash_Map[Child_Idx]\n",
    "#         Child_Datapoint = Child[1]\n",
    "\n",
    "#         # if child is already labelled then return\n",
    "#         if labels[Child_Idx] > 0:\n",
    "#             idx += 1\n",
    "#             continue\n",
    "            \n",
    "        \n",
    "#         neigh = NearestNeighbors(radius = Radius)\n",
    "#         neigh.fit(dataset)\n",
    "#         rng = neigh.radius_neighbors([Child_Datapoint])\n",
    "#         #print(rng[0]) \n",
    "\n",
    "#         Density_Child = sum(rng[0][0])/len(rng[0][0])\n",
    "\n",
    "#         # if dis-density threshold criteria is satisfied\n",
    "#         if Density_Parent not in list_of_parents:\n",
    "#             list_of_parents.append(Density_Parent)\n",
    "#         data_parent = np.array(list_of_parents, dtype=np.float64)\n",
    "#         weighted_density = numpy_ewma_vectorized_v2(data_parent, alpha_weight_avg)\n",
    "#         print(\"Density of Parent =\", weighted_density)\n",
    "#         print(\"Density of Child = \", Density_Child)\n",
    "#         numerator = abs(weighted_density - Density_Child)\n",
    "#         denominator = weighted_density\n",
    "#         print(\"Weighted Density =\", numerator/denominator)\n",
    "        \n",
    "#         if numerator/denominator <= thresh:\n",
    "#             labels[Child_Idx] = cluster_num\n",
    "#             can_form_cluster[0] = [True]\n",
    "#             NovelClus(Child, Hash_Map, 0, dataset, K, list_of_parents, labels, thresh, cluster_num, can_form_cluster)\n",
    "\n",
    "#         # else term Child as \"Anomaly\" and return\n",
    "#         else:\n",
    "#             #print(\"\\n\")\n",
    "#             labels[Child_Idx] = -1\n",
    "#             idx += 1\n",
    "#             continue\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "BFS based Approach: \n",
    "\n",
    "It takes care of the maximum recursion depth constraint in Python programming Language.\n",
    "This however may take more time than DFS\n",
    "\"\"\"\n",
    "\n",
    "def NovelClus_FF(Parent, Hash_Map, dataset, K, list_of_parents_queue, labels, thresh, cluster_num, can_form_cluster, alpha_weight_avg):\n",
    "    ##print(\"Cluster =\", cluster_num)\n",
    "    \n",
    "    # We make use of a Deque for both side operation\n",
    "    Queue = deque()\n",
    "    \n",
    "    # Append from left the Parent Node\n",
    "    Queue.appendleft(Parent)\n",
    "    \n",
    "    \n",
    "    # While Queue is not Null:\n",
    "        # Queue extracts the first Node\n",
    "        # We check if k children of Node satisfies the density criterion\n",
    "        # if a child satisfies, it is pushed into the Queue from the end; else it is termed -1 and left\n",
    "    \n",
    "    while Queue:\n",
    "        # Pop from the right end\n",
    "        Parent = Queue.pop()\n",
    "        \n",
    "            \n",
    "        # Getting important information from Parent\n",
    "        Density_Parent = Parent[0]\n",
    "        Radius = Parent[2]\n",
    "        #Child_Idx = Parent[3][idx] \n",
    "        Child_Idx_array = Parent[3]   # [94,  32, 117, .... 100, 118]\n",
    "        #Child = Hash_Map[Child_Idx]\n",
    "        #Child_Datapoint = Child[1]\n",
    "        \n",
    "        # Update the list_of_parents for the weighted moving average\n",
    "        list_of_parents = list_of_parents_queue.pop() + [Density_Parent] \n",
    "        #print(\"LoP =\", list_of_parents)\n",
    "        \n",
    "        \n",
    "        Child_array = []\n",
    "        Child_Datapoint_array = []\n",
    "        for Child_Idx in Child_Idx_array:\n",
    "            Child_array.append(Hash_Map[Child_Idx])\n",
    "            Child_Datapoint_array.append(Hash_Map[Child_Idx][1])  \n",
    "        \n",
    "        \n",
    "        # For each Parent find which child satiesfies the density threshold criterion\n",
    "        ##print(\"Number of Children Nodes =\", len(Child_Idx_array), \"\\n\")\n",
    "        for i in range(len(Child_Idx_array)):\n",
    "            \n",
    "            Child = Child_array[i]\n",
    "            Child_Idx = Child[-1]             ##Child_Idx_array[i]\n",
    "            Child_Datapoint = Child[1]        ##Child_Datapoint_array[i]\n",
    "            \n",
    "            # Base case: if child is already labelled in cluster then ignore\n",
    "            if labels[Child_Idx] > 0:\n",
    "                #print(\"Child is already labelled!!\")\n",
    "                continue\n",
    "                \n",
    "            neigh = NearestNeighbors(radius = Radius)\n",
    "            neigh.fit(dataset)\n",
    "            rng = neigh.radius_neighbors([Child_Datapoint])\n",
    "            #print(rng[0])\n",
    "            \n",
    "            Density_Child = sum(rng[0][0])/len(rng[0][0])\n",
    "            \n",
    "            #print(\"No. of Parent =\",len(list_of_parents))\n",
    "            \n",
    "            data_parent = np.array(list_of_parents) #, dtype=np.float64)\n",
    "            weighted_density = numpy_ewma_vectorized_v2(data_parent, alpha_weight_avg)\n",
    "            #print(\"Density of Parent =\", weighted_density)\n",
    "            #print(\"Density of Child = \", Density_Child)\n",
    "            numerator = abs(weighted_density - Density_Child)\n",
    "            denominator = weighted_density\n",
    "            #print(\"Weighted Density =\", numerator/denominator)\n",
    "            \n",
    "            \n",
    "            if numerator/denominator <= thresh:\n",
    "                labels[Child_Idx] = cluster_num\n",
    "                \n",
    "                list_of_parents_queue.appendleft(list_of_parents)\n",
    "                       \n",
    "                # Update the information for this child\n",
    "                Child[0] = Density_Child\n",
    "                Child[2] = max(rng[0][0])\n",
    "                Child[3] = rng[1][0]\n",
    "                \n",
    "                Queue.appendleft(Child)\n",
    "                \n",
    "                # Update the hashmap OLD\n",
    "                ##Hash_Map[Child[-1]] = Child\n",
    "                \n",
    "                # Update the hashmap NEW\n",
    "                temp_dict = {Child[-1]:Child}\n",
    "                Hash_Map.update(temp_dict)\n",
    "                ##################################\n",
    "                \n",
    "            \n",
    "                # The Parent can form a cluster of its own\n",
    "                can_form_cluster[0] = [True]\n",
    "                \n",
    "                \n",
    "            else:\n",
    "                #print(\"Possible Anomaly!!\")\n",
    "                labels[Child_Idx] = -1\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_nbrs(data1, idx, K, algo): # 70\n",
    "    nbrs_ddcal = NearestNeighbors(n_neighbors=K, algorithm=algo).fit(data1)\n",
    "    distances_ddcal, indices_ddcal = nbrs_ddcal.kneighbors(data1)\n",
    "\n",
    "    density_mod = []\n",
    "    density_mod.append([sum(distances_ddcal[idx])/(K-1), data1[idx], max(distances_ddcal[idx]), indices_ddcal[idx][1:], idx])\n",
    "    \n",
    "    density_mod.append([sum(distances_ddcal[i])/(K-1), data1[i], max(distances_ddcal[i]), indices_ddcal[i][1:], i])\n",
    "    # density_cpy.append([sum(distances_ddcal[i])/(K-1), data1[i], max(distances_ddcal[i]), indices_ddcal[i][1:], i])\n",
    "    \n",
    "    return density_mod[0] #, density_mod[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_idx = 0\n",
    "cluster_num = 0\n",
    "cluster_centers = []\n",
    "hold_cluster_val = cluster_num ####\n",
    "\n",
    "\n",
    "for data_idx in range(len(data1)):\n",
    "    \n",
    "    if (data_idx + 1) % 100 == 0:\n",
    "        print(\"**************************Computed {} Datapoints**************************\".format(data_idx+1))\n",
    "    idx = 0\n",
    "    list_of_parents_queue = deque()\n",
    "    list_of_parents_queue.appendleft([])\n",
    "    \n",
    "    if labels[density[data_idx][-1]] == 0: #<= 0:\n",
    "        cluster_num += 1\n",
    "        labels[density[data_idx][-1]] = cluster_num\n",
    "        can_form_cluster = [False]\n",
    "        \n",
    "        \n",
    "        # Update information for the Parent node\n",
    "        #   0                         1         2        3                    4\n",
    "        # Distance-Density Value; Datapoint; Radius; k-nearest neighbors; idx_number\n",
    "        \n",
    "        \n",
    "        # print(density[data_idx][-1])\n",
    "        # break\n",
    "        \n",
    "        ## Modification ##############\n",
    "        # print(density[data_idx])\n",
    "        # print(\"\\n\\n\")\n",
    "        K_mod = LLE(data1, K, density[data_idx][-1])\n",
    "        print(\"K modified: \",K_mod)\n",
    "        \n",
    "        # Old One\n",
    "        ##density[data_idx], Hash_Map[density[data_idx][-1]] = update_nbrs(data1, density[data_idx][-1], K_mod, algo)\n",
    "        # print(density[data_idx])\n",
    "        # break\n",
    "        #########################################\n",
    "        \n",
    "        # New One\n",
    "        density[data_idx] = update_nbrs(data1, density[data_idx][-1], K_mod, algo)\n",
    "        temp_dict = {density[data_idx][-1]:density[data_idx]}\n",
    "        #########################################\n",
    "        \n",
    "        NovelClus_FF(density[data_idx], Hash_Map, data1, K_mod, list_of_parents_queue, labels, thresh, cluster_num, can_form_cluster, alpha_weight_avg)\n",
    "        \n",
    "        \n",
    "        # Visualization\n",
    "        if Dimension <= 2:\n",
    "            plt.figure(figsize=(8,8))\n",
    "            for i in range(len(data1)):\n",
    "                if labels[i] == cluster_num:\n",
    "                    plt.scatter(data1[i][0],data1[i][1],c='pink')\n",
    "                elif labels[i] == -1:\n",
    "                    plt.scatter(data1[i][0],data1[i][1], c='yellow')\n",
    "                else:\n",
    "                    plt.scatter(data1[i][0], data1[i][1],c='blue')\n",
    "            plt.scatter(density[data_idx][1][0], density[data_idx][1][1], c='red',s=200, marker = '+')\n",
    "            plt.show()\n",
    "\n",
    "         \n",
    "        # If the last Parent Node cannot form a clusterof its own then it is a part of a cluster with only 1 datapoint\n",
    "        # which means \" a possible Anomaly\"\n",
    "        # then change its label to -1 and reduce the cluster num by 1\n",
    "        if can_form_cluster[0] == False:\n",
    "            labels[density[data_idx][-1]] = -1\n",
    "            cluster_num = hold_cluster_val  ####\n",
    "        else:\n",
    "            cluster_centers.append([density[data_idx][1].tolist(), cluster_num])\n",
    "            hold_cluster_val += 1  ####\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following print statements can be uncommented for better intuition\n",
    "# print(cluster_centers)\n",
    "# print(\"\\n\", len(cluster_centers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function corrects the wrong clustering\n",
    "def Correcting_wrong_clustering(labels, cluster_centers, data1):  \n",
    "    cluster_labels = []\n",
    "    for key in labels:\n",
    "        cluster_labels.append(labels[key])\n",
    "    \n",
    "    Hash_Map_2 = dict()\n",
    "    X_label = dict()\n",
    "    \n",
    "    for i in range(len(data1)): \n",
    "        if labels[i] == -1:\n",
    "            continue\n",
    "        if labels[i] in Hash_Map_2:\n",
    "            Hash_Map_2[labels[i]] += 1\n",
    "            X_label[labels[i]] = X_label[labels[i]] + [data1[i].tolist()]\n",
    "        else:\n",
    "            Hash_Map_2[labels[i]] = 1\n",
    "            X_label[labels[i]] = [data1[i].tolist()]\n",
    "            \n",
    "    #print(\"\\nHASH MAP\\n\", Hash_Map_2)\n",
    "\n",
    "    for key in Hash_Map_2:\n",
    "        temp = Hash_Map_2[key]\n",
    "        for idx in range(len(cluster_centers)):\n",
    "            if cluster_centers[idx][1] == key:\n",
    "                break\n",
    "        Hash_Map_2[key] = [temp, cluster_centers[idx][0]]\n",
    "        \n",
    "        \n",
    "    # Function to find Intra Cluster Distance    \n",
    "    def IntraClusterMetric(X):\n",
    "        Intra_Clus_Dist = 0\n",
    "        for i in range(len(X)):\n",
    "            Intra_Clus_Dist += (sum(euclidean_distances(X, [X[i]]))[0])/(len(X)-1)\n",
    "        return Intra_Clus_Dist/(len(X)-1)\n",
    "           \n",
    "    \n",
    "    for key in X_label:\n",
    "        X = X_label[key]\n",
    "        Intra_metric = IntraClusterMetric(X)\n",
    "        Hash_Map_2[key] = Hash_Map_2[key] + [Intra_metric]  \n",
    "    \n",
    "    #print(\"Hash_Map_2\\n\",Hash_Map_2,\"\\n\")\n",
    "    \n",
    "    clusnum_card = sorted(Hash_Map_2.items(), key=lambda kv:(kv[1], kv[0]), reverse = False)\n",
    "    cluster_center_new = []\n",
    "    for ele in clusnum_card:\n",
    "        cluster_center_new.append(np.array(ele[1][1]))   \n",
    "\n",
    "    ## Following print statements can be uncommented for better intuition\n",
    "    # print(Hash_Map_2)\n",
    "    # print(\"\\nAfter Sorting \")\n",
    "    # print(clusnum_card)\n",
    "    # print(\"\\nOnly cluster centers\")\n",
    "    # print(cluster_center_new)\n",
    "    \n",
    "    \n",
    "    K1 = len(cluster_centers)\n",
    "    nbrs = NearestNeighbors(n_neighbors=K1, algorithm='auto').fit(cluster_center_new)\n",
    "    distances, indices = nbrs.kneighbors(cluster_center_new)\n",
    "    #print(indices,\"\\n\")\n",
    "    #print(distances)\n",
    "\n",
    "    \n",
    "    def GetCutoffDistMod(cluster_centers):\n",
    "        percent = 0.7\n",
    "        i = 0\n",
    "        j = 1\n",
    "        Count = 0\n",
    "        Sum = 0\n",
    "        while i < len(cluster_centers)-1:\n",
    "            while j < len(cluster_centers):\n",
    "                Sum += math.dist(cluster_centers[i], cluster_centers[j])\n",
    "                j += 1\n",
    "                Count += 1\n",
    "            i += 1\n",
    "            j = i + 1\n",
    "        Count = max(Count, 1)\n",
    "        return math.ceil(Sum/Count)*percent\n",
    "    \n",
    "    \n",
    "    #print(clusnum_card)\n",
    "    \n",
    "    #print(\"\\n\\n\")\n",
    "    dist_val = GetCutoffDistMod(cluster_center_new)\n",
    "    val2 = 20\n",
    "    val3 = 15\n",
    "    percent1 = 0.8\n",
    "    percent2 = 0.16\n",
    "    percent3 = 0.10\n",
    "    percent4 = 0.02\n",
    "\n",
    "    # Format of clus_to_change: [curr_clus, clus_to_be]\n",
    "    clus_to_change = []\n",
    "    for i in range(len(clusnum_card)-1):\n",
    "        # We are not considering till the last term since,\n",
    "        # last term is the most dense and biggest among all \n",
    "        # the other clusters. It cannot be merged with others\n",
    "        \n",
    "        j = 1\n",
    "        cardinality_of_self = Hash_Map_2[clusnum_card[i][0]][0]\n",
    "        Intra_Clus_Self = Hash_Map_2[clusnum_card[i][0]][2]\n",
    "        cardinality_of_neighbor = Hash_Map_2[clusnum_card[indices[i][j]][0]][0]\n",
    "        Intra_Clus_Neigh = Hash_Map_2[clusnum_card[indices[i][j]][0]][2]\n",
    "        \n",
    "        #print(cardinality_of_self, cardinality_of_neighbor)\n",
    "        while cardinality_of_self >= cardinality_of_neighbor and j < len(clusnum_card)-1:\n",
    "            #print(\"YES\")\n",
    "            j += 1\n",
    "            #print(\"j=\",j)\n",
    "            cardinality_of_neighbor = Hash_Map_2[clusnum_card[indices[i][j]][0]][0]  #-1\n",
    "            Intra_Clus_Neigh = Hash_Map_2[clusnum_card[indices[i][j]][0]][2]\n",
    "\n",
    "        # cardinality_of_self = Hash_Map_2[clusnum_card[i][0]]\n",
    "        # cardinality_of_neighbor = Hash_Map_2[clusnum_card[indices[i][j]][0]][0]\n",
    "        # print(cardinality_of_neighbor)\n",
    "    \n",
    "        ## Following print statements can be uncommented for better intuition\n",
    "        # print(\"Cluster number\", clusnum_card[i][0], \"Cardinality \", Hash_Map_2[clusnum_card[i][0]][0])\n",
    "        # print(\"Neighbor Cluster number\", clusnum_card[indices[i][j]][0], \"Cardinality \", Hash_Map_2[clusnum_card[indices[i][j]][0]][0])\n",
    "        # print(\"Intra_Clus_Self\", Intra_Clus_Self, \"Intra_Clus_Neigh \", Intra_Clus_Neigh)\n",
    "        # print(\"Intra Val Lower =\", (Intra_Clus_Neigh - percent1*Intra_Clus_Neigh), \"Intra Val Higher =\", (Intra_Clus_Neigh + percent1*Intra_Clus_Neigh))\n",
    "        # print(\"Percent1*CNeigh\", int(percent1*cardinality_of_neighbor))\n",
    "        # print(\"Percent2 =\", (cardinality_of_self/cardinality_of_neighbor))\n",
    "        # print(\"val3\", val3)\n",
    "        # print(\"Or condition\", abs(cardinality_of_self - cardinality_of_neighbor), \"val2\", val2)\n",
    "        # print(\"Distances between self and neighbor\", int(round(distances[i][j],0)), \"Dist_val\", dist_val)\n",
    "        \n",
    "        \n",
    "        # Using a boolean merge to keep a track if the self cluster is merged or not\n",
    "        # Unless and until the cluster is merged, it is processed through every if condition\n",
    "        merged = False\n",
    "        \n",
    "        # If cardinality of self and cardinality of neighbor are both less than a given threshold\n",
    "        # where the threshold is very very small; say 10\n",
    "        if merged == False:\n",
    "            if cardinality_of_self <= val2 and cardinality_of_neighbor <= val2:\n",
    "                # Distance checking here isn't necessary\n",
    "                curr_clus = clusnum_card[i][0]\n",
    "                clus_to_be = clusnum_card[indices[i][j]][0]\n",
    "                clus_to_change.append([curr_clus, clus_to_be])\n",
    "                Hash_Map_2[clusnum_card[indices[i][j]][0]][0] += cardinality_of_self\n",
    "                #print(\"Merged 1st condition\")\n",
    "                merged = True  # made True so that we donot need to process it over other if conditions\n",
    "        \n",
    "        # If cardinality of self is less than 16% of cardinality of neighbor \n",
    "        if merged == False:\n",
    "            if (cardinality_of_self/cardinality_of_neighbor) <= percent2 and cardinality_of_self <= val3:\n",
    "                if int(round(distances[i][j],0)) <= dist_val:\n",
    "                    curr_clus = clusnum_card[i][0]\n",
    "                    clus_to_be = clusnum_card[indices[i][j]][0]\n",
    "                    clus_to_change.append([curr_clus, clus_to_be])\n",
    "                    Hash_Map_2[clusnum_card[indices[i][j]][0]][0] += cardinality_of_self\n",
    "                    #print(\"Merged 2nd condition\")\n",
    "                    merged = True  # made True so that we donot need to process it over other if conditions\n",
    "                \n",
    "        # If Cardinality of self > val3 but is less than percent6 of the cardinality of neighbor\n",
    "        if merged == False:\n",
    "            if val3 < cardinality_of_self <= val3*2.7 and cardinality_of_self/cardinality_of_neighbor <= percent3:\n",
    "                Intra_Upper_Limit = Intra_Clus_Neigh + percent1*Intra_Clus_Neigh\n",
    "                Intra_Lower_Limit = (Intra_Clus_Neigh - percent1*Intra_Clus_Neigh)\n",
    "                if (Intra_Lower_Limit <= Intra_Clus_Self <= Intra_Upper_Limit and int(round(distances[i][j],0)) <= dist_val):\n",
    "                    curr_clus = clusnum_card[i][0]\n",
    "                    clus_to_be = clusnum_card[indices[i][j]][0]\n",
    "                    clus_to_change.append([curr_clus, clus_to_be])\n",
    "                    Hash_Map_2[clusnum_card[indices[i][j]][0]][0] += cardinality_of_self\n",
    "                    #print(\"Merged 3rd condition\")\n",
    "                    merged = True  # made True so that we donot need to process it over other if conditions          \n",
    "        \n",
    "        # If Cardinality of self is less than percent4 of the cardinality of neighbor; where percent4 ~ 2 percent\n",
    "        if merged == False:\n",
    "            if cardinality_of_self/cardinality_of_neighbor <= percent4:\n",
    "                # Here we don't need to check the distance and intra cluster density;\n",
    "                # The self cluster is very very small compared to the neighbor cluster.\n",
    "                # So just merge them\n",
    "                curr_clus = clusnum_card[i][0]\n",
    "                clus_to_be = clusnum_card[indices[i][j]][0]\n",
    "                clus_to_change.append([curr_clus, clus_to_be])\n",
    "                Hash_Map_2[clusnum_card[indices[i][j]][0]][0] += cardinality_of_self\n",
    "                # print(\"Merged 4th condition\")\n",
    "                merged = True  # made True so that we donot need to process it over other if conditions      \n",
    "        \n",
    "        if merged == False: # Some merges by this logic as well\n",
    "            if (cardinality_of_self <= cardinality_of_neighbor and cardinality_of_self <= val3) or (abs(cardinality_of_self - cardinality_of_neighbor) <= val2 and cardinality_of_self <= val3):\n",
    "                Intra_Upper_Limit = Intra_Clus_Neigh + percent1*Intra_Clus_Neigh\n",
    "                Intra_Lower_Limit = (Intra_Clus_Neigh - percent1*Intra_Clus_Neigh)\n",
    "                if (Intra_Lower_Limit <= Intra_Clus_Self <= Intra_Upper_Limit and int(round(distances[i][j],0)) <= dist_val):\n",
    "                    curr_clus = clusnum_card[i][0]\n",
    "                    clus_to_be = clusnum_card[indices[i][j]][0]\n",
    "                    clus_to_change.append([curr_clus, clus_to_be])\n",
    "                    Hash_Map_2[clusnum_card[indices[i][j]][0]][0] += cardinality_of_self\n",
    "                    # print(\"Merged 5th condition\")\n",
    "                    merged = True  # made True so that we donot need to process it over other if conditions      \n",
    "\n",
    "        # print(\"\\n\")\n",
    "    print(\"\\nChanged Clusters are: [initial cluster --> New Cluster]\")\n",
    "    print(clus_to_change)\n",
    "    \n",
    "    \n",
    "    for key in labels:\n",
    "        for item in clus_to_change:\n",
    "            if labels[key] == item[0]:\n",
    "                labels[key] = item[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Correct the wrong clusters\n",
    "Correcting_wrong_clustering(labels, cluster_centers, data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This returns cluster numbers in a non-missing sorted order\n",
    "# Before this function many clusters may be removed in the previous step\n",
    "# So, to keep everything ordered, this function makes sure all clusters have a number\n",
    "# and the number starts from 1 all the way to n without any missing number in between\n",
    "def get_count_in_label(labels):\n",
    "    table = {value:0 for value in labels}\n",
    "    for ele in labels:\n",
    "        table[ele] += 1\n",
    "    key_list = table.keys()\n",
    "    #sorted(key_list, reverse = False)\n",
    "    for ele in sorted(key_list, reverse = False):\n",
    "        print(ele,\":\",table[ele])\n",
    "\n",
    "def get_label_list(hash_map):\n",
    "    label_list = []\n",
    "    for key in hash_map.keys():\n",
    "        if hash_map[key] > 0:\n",
    "            label_list.append(hash_map[key])\n",
    "        #else:\n",
    "        #    get_labels.append(hash_map[key])\n",
    "    #print(get_labels)\n",
    "    return label_list    \n",
    "\n",
    "        \n",
    "def find_missing_idx(hash_map):\n",
    "    max_idx = float(\"-inf\")\n",
    "    for key in hash_map:\n",
    "        if hash_map[key] > max_idx:\n",
    "            max_idx = hash_map[key]\n",
    "    #max_idx = max(hash_map.keys())\n",
    "    \n",
    "    min_idx = float(\"inf\")\n",
    "    for key in hash_map:\n",
    "        if hash_map[key] < min_idx and hash_map[key] != -1:\n",
    "            min_idx = hash_map[key]\n",
    "    #min_idx = 0\n",
    "\n",
    "    ########### print(min_idx, max_idx)                     \n",
    "    \n",
    "    label_list = get_label_list(hash_map)\n",
    "\n",
    "    missing_idx = []\n",
    "    count = 1\n",
    "    while count <= max_idx:\n",
    "        if count not in label_list:\n",
    "            missing_idx.append(count)\n",
    "        count += 1\n",
    "\n",
    "    ########## print(missing_idx)  \n",
    "    return missing_idx\n",
    "\n",
    "def correct_the_idx(hash_map): #labels):\n",
    "    missing_idx = find_missing_idx(hash_map) #labels)\n",
    "    \n",
    "    # Can be uncommented for better intuition\n",
    "    # print(missing_idx)\n",
    "    \n",
    "    while missing_idx:\n",
    "        for i in missing_idx:\n",
    "            for key in hash_map:# in range(len(labels)):\n",
    "                if hash_map[key] == i + 1: #labels[j] == i+1:\n",
    "                    hash_map[key] = hash_map[key] - 1 #labels[j] = labels[j] - 1\n",
    "\n",
    "        #get_count_in_label(labels)\n",
    "        \n",
    "        #print(\"\\n\")\n",
    "        #coorect_the_idx(labels)\n",
    "        \n",
    "        missing_idx = find_missing_idx(hash_map) #labels)\n",
    "\n",
    "    \n",
    "#missing_idx = correct_the_label_idx(labels)\n",
    "#print(missing_idx)\n",
    "\n",
    "#get_count_in_label(labels)\n",
    "# print(\"\\n\\n\")\n",
    "correct_the_idx(labels)\n",
    "#get_count_in_label(labels)\n",
    "#find_missing_idx(hash_map)\n",
    "#print(hash_map)\n",
    "\n",
    "#label_l = get_label_list(hash_map)\n",
    "#get_count_in_label(label_l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = []\n",
    "anomaly_list = []\n",
    "max_val = float(\"-inf\")\n",
    "for key in labels:\n",
    "    if max_val < labels[key]:\n",
    "        max_val = labels[key]\n",
    "    if labels[key] == -1:\n",
    "        anomaly_list.append(data1[key])\n",
    "    else:\n",
    "        label_list.append(labels[key])\n",
    "cluster_num = max_val\n",
    "#print(label_list)\n",
    "print(\"\\n------------ Before Outlier Post Processing ------------\")\n",
    "print(\"Total number of datapoints =\", len(data1))\n",
    "print(\"Number of Non-Anomalies =\", len(label_list))\n",
    "print(\"Number of Anomalies =\", len(anomaly_list))\n",
    "print(\"Number of Cluster(s) =\", cluster_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if Dimension <= 2:\n",
    "    plt.figure(figsize=(8,8))\n",
    "    \n",
    "    color = ['red', 'aqua', 'yellow', 'magenta', 'black', 'darkgreen', 'blue', 'gray', 'purple', 'lime', 'maroon', 'gold', 'deeppink', 'silver', 'lawngreen', 'pink', 'navy', 'blueviolet', 'turquoise', 'dodgerblue', 'navajowhite', 'khaki', 'darkslateblue', 'darkseagreen', 'mediumvioletred', 'palevioletred', 'cornflowerblue', 'plum', 'steelblue', 'lightcoral']\n",
    "\n",
    "    if cluster_num <= len(color):\n",
    "        for i in range(len(data1)):\n",
    "            if labels[i] != -1:\n",
    "                plt.scatter(data1[i][0], data1[i][1], c=color[labels[i]-1])\n",
    "            else:\n",
    "                plt.scatter(data1[i][0],data1[i][1],c='green',marker='*',s=150)\n",
    "    else:\n",
    "        for i in range(len(data1)):\n",
    "            if labels[i] == -1:\n",
    "                plt.scatter(data1[i][0],data1[i][1],c='green',marker='*',s=150)\n",
    "            else:\n",
    "                plt.scatter(data1[i][0], data1[i][1],c='blue', label='Datapoints')\n",
    "\n",
    "    plt.xlabel('X-Coordinates')\n",
    "    plt.ylabel('Y-Coordinates')\n",
    "    plt.title('Datapoints Visualization Before Outlier Post Processing')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OutlierPostProcessing(K, algo, data1, labels, percent5):\n",
    "    # Select a predetermined value of K\n",
    "    # Find the K nearest Neighbors of an outlier\n",
    "    # For each of those neighbors:\n",
    "    #     Find their Intracluster distance with the same K value\n",
    "    #     If the Intracluster distance of outlier is within +-5% of the intracuster distance of the neighbor,\n",
    "    #     and the neighbor point is not an outlier, \n",
    "    #     then consider the outlier to be a part of the smae cluster that the neighbor point belongs.\n",
    "    \n",
    "    nbrs = NearestNeighbors(n_neighbors=K, algorithm=algo).fit(data1)\n",
    "    distances, indices = nbrs_ddcal.kneighbors(data1)\n",
    "    \n",
    "    for i in range(len(data1)):\n",
    "        if labels[i] == -1:\n",
    "            # Predicted Outlier, so find its K nearest neighbors\n",
    "            PredOutlierNNidx = indices[i]\n",
    "            PredOutlierNN = []\n",
    "            for idx in PredOutlierNNidx:\n",
    "                PredOutlierNN.append(data1[idx])\n",
    "            # Find the intracluster distance for the Predicted Outlier\n",
    "            PredOutlierICD = IntraClusterDistance(PredOutlierNN)\n",
    "            # print(\"Outlier\", i, PredOutlierICD)\n",
    "            \n",
    "            \n",
    "            # For each of the outlier's K neighbors\n",
    "            for idx in PredOutlierNNidx[1:]:\n",
    "                \n",
    "                # Check if the neighbor is an outlier or inlier\n",
    "                if labels[idx] != -1:\n",
    "                    # print(\"Not an Outlier\")\n",
    "                    NNeighboridx = indices[idx]\n",
    "                    NNeighbor = []\n",
    "                    for idx1 in NNeighboridx:\n",
    "                        NNeighbor.append(data1[idx1])\n",
    "                        \n",
    "                    # Find each neighbor's IntraClusterDistance(ICD)\n",
    "                    NNICD = IntraClusterDistance(NNeighbor)\n",
    "                    #print(\"NNICD: \", NNICD)\n",
    "                    \n",
    "                    # Check if outlier's ICD is about +-5% of neighbor's ICD\n",
    "                    # print(\"Lower: \",(NNICD-percent5*NNICD), \"Upper: \",(NNICD+percent5*NNICD), \"Self: \",PredOutlierICD)\n",
    "                    if (NNICD-percent5*NNICD) <= PredOutlierICD <= (NNICD+percent5*NNICD):\n",
    "                        # Change the label of the Predicted Outlier 'i' to the \n",
    "                        # label of the 1st Nearest Neighbor which satifies the condition\n",
    "                        labels[i] = labels[idx]\n",
    "                        # print(\"Changed\\n\")\n",
    "                        break\n",
    "\n",
    "            \n",
    "def IntraClusterDistance(X):\n",
    "        Intra_Clus_Dist = 0\n",
    "        for i in range(len(X)):\n",
    "            Intra_Clus_Dist += (sum(euclidean_distances(X, [X[i]]))[0])/(len(X)-1)\n",
    "        return Intra_Clus_Dist/(len(X)-1)\n",
    "    \n",
    "\n",
    "\n",
    "KVal = int(0.03*len(data1)) #5 \n",
    "percent5 = 0.01\n",
    "# KVal = 25\n",
    "# percent5 = 0.06\n",
    "OutlierPostProcessing(KVal, algo, data1, labels, percent5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = []\n",
    "anomaly_list = []\n",
    "max_val = float(\"-inf\")\n",
    "for key in labels:\n",
    "    if max_val < labels[key]:\n",
    "        max_val = labels[key]\n",
    "    if labels[key] == -1:\n",
    "        anomaly_list.append(data1[key])\n",
    "    else:\n",
    "        label_list.append(labels[key])\n",
    "cluster_num = max_val\n",
    "#print(label_list)\n",
    "print(\"\\n------------ After Outlier Post Processing ------------\")\n",
    "print(\"Total number of datapoints =\", len(data1))\n",
    "print(\"Number of Non-Anomalies =\", len(label_list))\n",
    "print(\"Number of Anomalies =\", len(anomaly_list))\n",
    "print(\"Number of Cluster(s) =\", cluster_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Dimension <= 2:\n",
    "    plt.figure(figsize=(8,8))\n",
    "\n",
    "    color = ['red', 'aqua', 'yellow', 'magenta', 'black', 'darkgreen', 'blue', 'gray', 'purple', 'lime', 'maroon', 'gold', 'deeppink', 'silver', 'lawngreen', 'pink', 'navy', 'blueviolet', 'turquoise', 'dodgerblue', 'navajowhite', 'khaki', 'darkslateblue', 'darkseagreen', 'mediumvioletred', 'palevioletred', 'cornflowerblue', 'plum', 'steelblue', 'lightcoral']\n",
    "\n",
    "    if cluster_num <= len(color):\n",
    "        for i in range(len(data1)):\n",
    "            if labels[i] != -1:\n",
    "                plt.scatter(data1[i][0], data1[i][1], c=color[labels[i]-1])\n",
    "            else:\n",
    "                plt.scatter(data1[i][0],data1[i][1],c='green',marker='*',s=150)\n",
    "    else:\n",
    "        for i in range(len(data1)):\n",
    "            if labels[i] == -1:\n",
    "                plt.scatter(data1[i][0],data1[i][1],c='green',marker='*',s=150)\n",
    "            else:\n",
    "                plt.scatter(data1[i][0], data1[i][1],c='blue', label='Datapoints')\n",
    "\n",
    "    plt.xlabel('X-Coordinates')\n",
    "    plt.ylabel('Y-Coordinates')\n",
    "    plt.title('Datapoints Visualization After Outlier Post Processing')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Clustering:\n",
    "    # Performance Evaluation\n",
    "    # Ground_Truth_data_path=r\"E:\\ISI\\GROUND TRUTHS CLUSTERING\\a2_gt.csv\"\n",
    "    with open(Ground_Truth_data_path, 'r') as f:\n",
    "        reader = csv.reader(f, delimiter=',')\n",
    "        data_f = list(reader)\n",
    "\n",
    "    # Clusters from Ground Truth Data\n",
    "    clustersFromGT = []\n",
    "    clustersFromGT.append(0)\n",
    "\n",
    "    count = 1\n",
    "    temp = []\n",
    "    temp.append(int(data_f[0][-1]))\n",
    "\n",
    "    for i in range(1, len(data_f)):\n",
    "        if int(data_f[i][-1]) != int(data_f[i-1][-1]) and int(data_f[i][-1]) not in temp:\n",
    "            temp.append(int(data_f[i][-1]))\n",
    "            clustersFromGT.append(i)\n",
    "            count += 1\n",
    "    clustersFromGT.append(i+1)\n",
    "\n",
    "\n",
    "    def EvaluateClustering(labels_PC, n_e_val_0, data_path, clustersFromGT, data1, labels_Idxs):\n",
    "        with open(data_path, 'r') as f:\n",
    "            reader = csv.reader(f, delimiter=',')\n",
    "            data = list(reader)\n",
    "            data = np.array(data).astype(float)\n",
    "\n",
    "        d=2\n",
    "        row=len(data)\n",
    "        dimension=len(data[0])\n",
    "        neighbors=4\n",
    "        W=np.zeros((row, row))\n",
    "\n",
    "        for i in range(row):\n",
    "            D_i=np.array(data-data[i, :])\n",
    "            distance=(D_i**2).sum(1)\n",
    "            nearest_neighbor=np.argsort(distance)[1:(neighbors+1)]\n",
    "            D_nbrs=D_i[nearest_neighbor, :]\n",
    "            Q=np.dot(D_nbrs, D_nbrs.T)\n",
    "            t=np.trace(Q)\n",
    "            r=0.001*t\n",
    "            if(neighbors>=dimension):\n",
    "                Q=Q+(r*np.identity(neighbors))\n",
    "            w=np.linalg.solve(Q, np.ones(neighbors))\n",
    "            w=w/sum(w)\n",
    "            W[i, nearest_neighbor]=w\n",
    "\n",
    "        I=np.identity(row)\n",
    "        M=I-W\n",
    "\n",
    "        U, S, Vt = np.linalg.svd(M)\n",
    "        e_val=S**2\n",
    "        e_val_0=np.array(e_val<10**-20)\n",
    "\n",
    "\n",
    "        uniq=np.array(np.unique(labels_PC))\n",
    "        confusion_mat=np.zeros((n_e_val_0,n_e_val_0))\n",
    "        status=np.zeros((len(uniq)))\n",
    "        # print(\"\\nSTATUS_INIT\",status,\"\\n\")                                                                    \n",
    "\n",
    "        x = 0\n",
    "        y = 1\n",
    "        f = 0                       \n",
    "\n",
    "        get_cluster_from_ground_truth = clustersFromGT\n",
    "\n",
    "\n",
    "        while y < len(get_cluster_from_ground_truth):\n",
    "            cm_k=np.zeros((n_e_val_0))\n",
    "            start = get_cluster_from_ground_truth[x]\n",
    "            end = get_cluster_from_ground_truth[y]\n",
    "            pred_labels=labels_PC[start:end]\n",
    "            f += 1\n",
    "            #print(f)\n",
    "            while True:\n",
    "                maj_label=np.argmax(np.bincount(pred_labels))\n",
    "                if(status[maj_label]!=1):\n",
    "                    status[maj_label]=1\n",
    "                    break\n",
    "                pred_labels=pred_labels[pred_labels!=maj_label]\n",
    "                if len(pred_labels) == 0:\n",
    "                    break\n",
    "            cl_k_k=np.count_nonzero(np.array(pred_labels==maj_label))  # True Positive\n",
    "            cm_k[maj_label]=cl_k_k  \n",
    "            for i in range((n_e_val_0)): # n_e_val_0 = no.of clusters in dataset ? Till now 'NO'  \n",
    "                if (i != maj_label):\n",
    "                    cl=np.count_nonzero(np.array(pred_labels==uniq[i]))\n",
    "                    cm_k[i]=cl\n",
    "            confusion_mat[maj_label, :]=cm_k\n",
    "            x += 1\n",
    "            y += 1\n",
    "\n",
    "        # print(status,\"\\n\")\n",
    "\n",
    "        def precision(label, confusion_mat):\n",
    "            col=confusion_mat[:,label]\n",
    "            return confusion_mat[label, label]/col.sum()\n",
    "\n",
    "        def recall(label, confusion_mat):\n",
    "            row=confusion_mat[label,:]\n",
    "            return confusion_mat[label,label]/row.sum()\n",
    "\n",
    "        precision_arr=[]\n",
    "        recall_arr=[]\n",
    "        f_score=[]\n",
    "        g_mean=[]\n",
    "        for i in range(n_e_val_0):\n",
    "            p=precision(i, confusion_mat)\n",
    "            r=recall(i, confusion_mat)\n",
    "            f=2*p*r/(p+r)\n",
    "            g=np.sqrt(p*r)\n",
    "            precision_arr=np.append(precision_arr, p)\n",
    "            recall_arr=np.append(recall_arr, r)\n",
    "            f_score=np.append(f_score, f)\n",
    "            g_mean=np.append(g_mean, g)\n",
    "\n",
    "        f_score = f_score[np.logical_not(np.isnan(f_score))]\n",
    "        g_mean = g_mean[np.logical_not(np.isnan(g_mean))]\n",
    "\n",
    "        f_score=np.mean(f_score)\n",
    "        g_mean=np.mean(g_mean)\n",
    "        accuracy=(np.trace(confusion_mat)/len(data))*100\n",
    "\n",
    "        print(\"\\n***************************** PERFORMANCE MEASURE *****************************\")\n",
    "        print(\"Accuracy: \",accuracy,\"--------- f score:\",f_score,\"--------- g mean:\",g_mean)\n",
    "        print(\"DB Index =\", davies_bouldin_score(data1, labels_Idxs))\n",
    "        distances = pairwise_distances(data1)\n",
    "        print(\"Dunn Index =\", dunn(distances, labels_Idxs))\n",
    "        print('Silhouette Score =',silhouette_score(data1, labels_Idxs))\n",
    "        print('Calinski-Harabasz Index =',metrics.calinski_harabasz_score(data1, labels_Idxs))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Function to generate labels for getting DB, Dunn, Silhouette and Calinski-Harabasz Indexes\n",
    "    def GetLabelsForIdxs(labels):\n",
    "        get_labels_DB = []\n",
    "        for key in labels:\n",
    "            if labels[key] > 0:\n",
    "                get_labels_DB.append(labels[key]-1)\n",
    "            else:\n",
    "                get_labels_DB.append(labels[key])\n",
    "        return get_labels_DB\n",
    "\n",
    "    # Function to generate labels for getting the accurcay, f score and g mean\n",
    "    def GetLabelsForPC(labels):\n",
    "        get_labels_PC = []\n",
    "        for key in labels:\n",
    "            if labels[key] > 0:\n",
    "                get_labels_PC.append(labels[key]-1)\n",
    "        return get_labels_PC\n",
    "\n",
    "\n",
    "    get_labels_PC = GetLabelsForPC(labels)\n",
    "    labels_PC = np.array(get_labels_PC)\n",
    "    get_labels_DB = GetLabelsForIdxs(labels)\n",
    "    labels_Idxs = np.array(get_labels_DB)\n",
    "    EvaluateClustering(labels_PC, cluster_num, Ground_Truth_data_path, clustersFromGT, data1, labels_Idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Outlier_Detection:\n",
    "    def OutlierDetectionEvaluation(actual_labels, pred_labels, data1, labels_Idxs):\n",
    "        # confusion matrix in sklearn\n",
    "        from sklearn.metrics import confusion_matrix\n",
    "        from sklearn.metrics import classification_report\n",
    "\n",
    "        # actual values\n",
    "        actual = actual_labels #[1,0,0,1,0,0,1,0,0,1]\n",
    "        # predicted values\n",
    "        predicted = pred_labels #[1,0,0,1,0,0,0,1,0,0]\n",
    "\n",
    "        # confusion matrix\n",
    "        matrix = confusion_matrix(actual,predicted, labels=[1,0])\n",
    "        #print('Confusion matrix : \\n',matrix)\n",
    "\n",
    "        # outcome values order in sklearn\n",
    "        tp, fn, fp, tn = confusion_matrix(actual,predicted,labels=[1,0]).reshape(-1)\n",
    "        #print('Outcome values : \\n', tp, fn, fp, tn)\n",
    "\n",
    "        # classification report for precision, recall f1-score and accuracy\n",
    "        matrix = classification_report(actual,predicted,labels=[1,0])\n",
    "        print('Classification report : \\n',matrix)\n",
    "\n",
    "\n",
    "        print(\"\\n\")\n",
    "        print(\"DB Index =\", davies_bouldin_score(data1, labels_Idxs))\n",
    "        distances = pairwise_distances(data1)\n",
    "        print(\"Dunn Index =\", dunn(distances, labels_Idxs))\n",
    "        print('Silhouette Score =',silhouette_score(data1, labels_Idxs))\n",
    "        print('Calinski-Harabasz Index =',metrics.calinski_harabasz_score(data1, labels_Idxs))\n",
    "\n",
    "\n",
    "    def Actual_labels(Ground_Truth_data_path):\n",
    "        with open(Ground_Truth_data_path, 'r') as f:\n",
    "            actual_labels = np.genfromtxt(f, delimiter=',')\n",
    "        return actual_labels\n",
    "\n",
    "    def GetLabelsForOutlierDet(labels):\n",
    "        get_labels_OutlierDet = []\n",
    "        for key in labels:\n",
    "            if labels[key] != -1:\n",
    "                get_labels_OutlierDet.append(0)\n",
    "            else:\n",
    "                get_labels_OutlierDet.append(1)\n",
    "        return get_labels_OutlierDet\n",
    "\n",
    "    # Function to generate labels for getting DB, Dunn, Silhouette and Calinski-Harabasz Indexes\n",
    "    def GetLabelsForIdxs(labels):\n",
    "        get_labels_DB = []\n",
    "        for key in labels:\n",
    "            if labels[key] > 0:\n",
    "                get_labels_DB.append(labels[key]-1)\n",
    "            else:\n",
    "                get_labels_DB.append(labels[key])\n",
    "        return get_labels_DB\n",
    "\n",
    "\n",
    "    actual_labels = Actual_labels(Ground_Truth_data_path)\n",
    "    pred_labels = GetLabelsForOutlierDet(labels)\n",
    "    get_labels_DB = GetLabelsForIdxs(labels)\n",
    "    labels_Idxs = np.array(get_labels_DB)\n",
    "    OutlierDetectionEvaluation(actual_labels, pred_labels, data1, labels_Idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"time taken = \",time.time()-start,\" sec\")\n",
    "#print(\"Average density =\", sum(density)/len(data1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
